{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=\"NLTK stands for Natural Language Toolkit. It is a popular open-source Python library that provides a wide range of tools and resources for natural language processing (NLP). NLTK offers various functionalities and algorithms for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, parsing, semantic reasoning, and more.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'stands',\n",
       " 'for',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'open-source',\n",
       " 'Python',\n",
       " 'library',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'of',\n",
       " 'tools',\n",
       " 'and',\n",
       " 'resources',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'offers',\n",
       " 'various',\n",
       " 'functionalities',\n",
       " 'and',\n",
       " 'algorithms',\n",
       " 'for',\n",
       " 'tasks',\n",
       " 'such',\n",
       " 'as',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " ',',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=word_tokenize(document)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK NNP\n",
      "stands VBZ\n",
      "for IN\n",
      "Natural NNP\n",
      "Language NNP\n",
      "Toolkit NNP\n",
      ". .\n",
      "It PRP\n",
      "is VBZ\n",
      "a DT\n",
      "popular JJ\n",
      "open-source NN\n",
      "Python NNP\n",
      "library NN\n",
      "that WDT\n",
      "provides VBZ\n",
      "a DT\n",
      "wide JJ\n",
      "range NN\n",
      "of IN\n",
      "tools NNS\n",
      "and CC\n",
      "resources NNS\n",
      "for IN\n",
      "natural JJ\n",
      "language NN\n",
      "processing NN\n",
      "( (\n",
      "NLP NNP\n",
      ") )\n",
      ". .\n",
      "NLTK NNP\n",
      "offers VBZ\n",
      "various JJ\n",
      "functionalities NNS\n",
      "and CC\n",
      "algorithms NN\n",
      "for IN\n",
      "tasks NNS\n",
      "such JJ\n",
      "as IN\n",
      "tokenization NN\n",
      ", ,\n",
      "stemming VBG\n",
      ", ,\n",
      "lemmatization NN\n",
      ", ,\n",
      "part-of-speech JJ\n",
      "tagging NN\n",
      ", ,\n",
      "parsing NN\n",
      ", ,\n",
      "semantic JJ\n",
      "reasoning NN\n",
      ", ,\n",
      "and CC\n",
      "more JJR\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "tagging=pos_tag(tokens)\n",
    "for word,tag in tagging:\n",
    "    print(word,tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'stands',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " '.',\n",
       " 'popular',\n",
       " 'open-source',\n",
       " 'Python',\n",
       " 'library',\n",
       " 'provides',\n",
       " 'wide',\n",
       " 'range',\n",
       " 'tools',\n",
       " 'resources',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'offers',\n",
       " 'various',\n",
       " 'functionalities',\n",
       " 'algorithms',\n",
       " 'tasks',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'lemmatization',\n",
       " ',',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " ',',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the list of stop words\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "filtered_tokens=[token for token in tokens if token.lower() not in stop_words ]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# stemmer=PorterStemmer()\n",
    "# lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "# stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]\n",
    "# lemmatizer_tokens=[lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "# stemmed_tokens\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word = \"running\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "print(stemmed_word)  # Output: run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('Dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "# lemmatizer_tokens\n",
    "# preprocessed_document = \" \".join(lemmatizer_tokens)\n",
    "# preprocessed_document\n",
    "\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "text = \"The Dogs are barking\"\n",
    "word_token = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word_token)\n",
    "print(pos_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF values:\n",
      "  (0, 8)\t0.7071067811865476\n",
      "  (0, 4)\t0.7071067811865476\n",
      "  (1, 9)\t0.4403620672313486\n",
      "  (1, 6)\t0.4403620672313486\n",
      "  (1, 2)\t0.3349067026613031\n",
      "  (1, 0)\t0.4403620672313486\n",
      "  (1, 1)\t0.4403620672313486\n",
      "  (1, 8)\t0.3349067026613031\n",
      "  (2, 5)\t0.49047908420610337\n",
      "  (2, 3)\t0.49047908420610337\n",
      "  (2, 7)\t0.49047908420610337\n",
      "  (2, 2)\t0.3730219858594306\n",
      "  (2, 4)\t0.3730219858594306\n",
      "IDF values:\n",
      "daily : 1.6931471805599454\n",
      "do : 1.6931471805599454\n",
      "exercise : 1.2876820724517808\n",
      "for : 1.6931471805599454\n",
      "good : 1.2876820724517808\n",
      "health : 1.6931471805599454\n",
      "in : 1.6931471805599454\n",
      "is : 1.6931471805599454\n",
      "morning : 1.2876820724517808\n",
      "the : 1.6931471805599454\n",
      "Indexes: {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n"
     ]
    }
   ],
   "source": [
    "d0= \"good morning\"\n",
    "d1= \"do daily exercise in the morning\"\n",
    "d2= \"exercise is good for health\"\n",
    "series= [d0,d1,d2]\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "#tf-idf values\n",
    "res= tfidf.fit_transform(series)\n",
    "print('TF values:')\n",
    "print(res)\n",
    "\n",
    "#idf values\n",
    "print('IDF values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1,\":\",ele2)\n",
    "    \n",
    "print('Indexes:', tfidf.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
